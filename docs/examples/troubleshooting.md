# æ•…éšœæ’é™¤æŒ‡å—

æœ¬æ–‡æ¡£æä¾›äº†Locustæ€§èƒ½æµ‹è¯•æ¡†æ¶å¸¸è§é—®é¢˜çš„è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©æ‚¨å¿«é€Ÿå®šä½å’Œè§£å†³æµ‹è¯•è¿‡ç¨‹ä¸­é‡åˆ°çš„é—®é¢˜ã€‚

## ğŸ” é—®é¢˜è¯Šæ–­æµç¨‹

### å¿«é€Ÿè¯Šæ–­æ£€æŸ¥æ¸…å•

```bash
# 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose ps
systemctl status locust

# 2. æ£€æŸ¥ç«¯å£å ç”¨
netstat -tlnp | grep :8089
netstat -tlnp | grep :5557

# 3. æ£€æŸ¥æ—¥å¿—
tail -f logs/locust.log
docker-compose logs -f locust-master

# 4. æ£€æŸ¥èµ„æºä½¿ç”¨
top -p $(pgrep -f locust)
docker stats

# 5. æ£€æŸ¥ç½‘ç»œè¿æ¥
curl -I http://localhost:8089/
ping target-host
```

## ğŸš¨ å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

### 1. å¯åŠ¨å’Œè¿æ¥é—®é¢˜

#### é—®é¢˜ï¼šLocustæ— æ³•å¯åŠ¨

**ç—‡çŠ¶**
```
Error: No module named 'locust'
ImportError: cannot import name 'HttpUser' from 'locust'
```

**è§£å†³æ–¹æ¡ˆ**
```bash
# æ£€æŸ¥Pythonç¯å¢ƒ
python --version
pip list | grep locust

# é‡æ–°å®‰è£…ä¾èµ–
pip install --upgrade locust
pip install -r requirements.txt

# æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate
which python
```

#### é—®é¢˜ï¼šWorkeræ— æ³•è¿æ¥åˆ°Master

**ç—‡çŠ¶**
```
[ERROR] Failed to connect to the Locust master
ConnectionRefusedError: [Errno 111] Connection refused
```

**è¯Šæ–­æ­¥éª¤**
```python
# diagnostic_tools.py
import socket
import time

def check_master_connectivity(master_host, master_port=5557):
    """æ£€æŸ¥Masterè¿æ¥æ€§"""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(5)
        result = sock.connect_ex((master_host, master_port))
        sock.close()

        if result == 0:
            print(f"âœ… Master {master_host}:{master_port} is reachable")
            return True
        else:
            print(f"âŒ Master {master_host}:{master_port} is not reachable")
            return False
    except Exception as e:
        print(f"âŒ Connection test failed: {e}")
        return False

def diagnose_network_issues():
    """ç½‘ç»œé—®é¢˜è¯Šæ–­"""
    import subprocess

    # æ£€æŸ¥DNSè§£æ
    try:
        result = subprocess.run(['nslookup', 'locust-master'],
                              capture_output=True, text=True, timeout=10)
        print("DNS Resolution:")
        print(result.stdout)
    except Exception as e:
        print(f"DNS check failed: {e}")

    # æ£€æŸ¥è·¯ç”±
    try:
        result = subprocess.run(['traceroute', 'locust-master'],
                              capture_output=True, text=True, timeout=30)
        print("Network Route:")
        print(result.stdout)
    except Exception as e:
        print(f"Route check failed: {e}")

# ä½¿ç”¨è¯Šæ–­å·¥å…·
if __name__ == "__main__":
    check_master_connectivity("locust-master")
    diagnose_network_issues()
```

**è§£å†³æ–¹æ¡ˆ**
```bash
# 1. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
sudo ufw status
sudo iptables -L

# 2. æ£€æŸ¥Dockerç½‘ç»œ
docker network ls
docker network inspect locust_default

# 3. ä¿®å¤ç½‘ç»œé…ç½®
docker-compose down
docker network prune
docker-compose up -d

# 4. ä½¿ç”¨æ­£ç¡®çš„ä¸»æœºå
# åœ¨docker-compose.ymlä¸­ç¡®ä¿æœåŠ¡åç§°æ­£ç¡®
services:
  locust-master:
    environment:
      - LOCUST_MASTER_BIND_HOST=0.0.0.0
  locust-worker:
    environment:
      - LOCUST_MASTER_HOST=locust-master
```

### 2. æ€§èƒ½é—®é¢˜

#### é—®é¢˜ï¼šå“åº”æ—¶é—´å¼‚å¸¸é«˜

**ç—‡çŠ¶**
```
Average response time: 5000ms+
95th percentile: 10000ms+
```

**è¯Šæ–­å·¥å…·**
```python
# performance_analyzer.py
import time
import statistics
from locust import events

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        self.response_times = []
        self.slow_requests = []
        self.error_patterns = {}

    def analyze_response_time(self, response_time, request_name):
        """åˆ†æå“åº”æ—¶é—´"""
        self.response_times.append(response_time)

        # è®°å½•æ…¢è¯·æ±‚
        if response_time > 3000:  # 3ç§’ä»¥ä¸Šä¸ºæ…¢è¯·æ±‚
            self.slow_requests.append({
                'name': request_name,
                'response_time': response_time,
                'timestamp': time.time()
            })

    def get_performance_summary(self):
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if not self.response_times:
            return "No data available"

        return {
            'avg_response_time': statistics.mean(self.response_times),
            'median_response_time': statistics.median(self.response_times),
            'p95_response_time': self.percentile(self.response_times, 95),
            'p99_response_time': self.percentile(self.response_times, 99),
            'slow_requests_count': len(self.slow_requests),
            'slow_requests_percentage': len(self.slow_requests) / len(self.response_times) * 100
        }

    def percentile(self, data, p):
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        sorted_data = sorted(data)
        index = int(len(sorted_data) * p / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]

    def identify_bottlenecks(self):
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []

        # åˆ†ææ…¢è¯·æ±‚æ¨¡å¼
        slow_endpoints = {}
        for req in self.slow_requests:
            endpoint = req['name']
            if endpoint not in slow_endpoints:
                slow_endpoints[endpoint] = []
            slow_endpoints[endpoint].append(req['response_time'])

        # æ‰¾å‡ºæœ€æ…¢çš„ç«¯ç‚¹
        for endpoint, times in slow_endpoints.items():
            if len(times) > 5:  # è‡³å°‘5ä¸ªæ…¢è¯·æ±‚
                avg_time = statistics.mean(times)
                bottlenecks.append({
                    'endpoint': endpoint,
                    'avg_slow_time': avg_time,
                    'slow_count': len(times)
                })

        return sorted(bottlenecks, key=lambda x: x['avg_slow_time'], reverse=True)

# é›†æˆåˆ°Locustæµ‹è¯•ä¸­
analyzer = PerformanceAnalyzer()

@events.request.add_listener
def on_request(request_type, name, response_time, response_length, exception, context, **kwargs):
    if not exception:
        analyzer.analyze_response_time(response_time, name)

@events.test_stop.add_listener
def on_test_stop(environment, **kwargs):
    summary = analyzer.get_performance_summary()
    bottlenecks = analyzer.identify_bottlenecks()

    print("\n=== Performance Analysis ===")
    print(f"Average Response Time: {summary['avg_response_time']:.2f}ms")
    print(f"95th Percentile: {summary['p95_response_time']:.2f}ms")
    print(f"Slow Requests: {summary['slow_requests_count']} ({summary['slow_requests_percentage']:.2f}%)")

    if bottlenecks:
        print("\n=== Performance Bottlenecks ===")
        for bottleneck in bottlenecks[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"- {bottleneck['endpoint']}: {bottleneck['avg_slow_time']:.2f}ms avg, {bottleneck['slow_count']} slow requests")
```

**è§£å†³æ–¹æ¡ˆ**
```python
# 1. ä¼˜åŒ–è¿æ¥æ± 
from locust import HttpUser
import requests.adapters

class OptimizedUser(HttpUser):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # å¢åŠ è¿æ¥æ± å¤§å°
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=20,
            pool_maxsize=50,
            max_retries=3
        )
        self.client.mount("http://", adapter)
        self.client.mount("https://", adapter)

        # è®¾ç½®åˆç†çš„è¶…æ—¶
        self.client.timeout = (10, 60)  # è¿æ¥è¶…æ—¶10sï¼Œè¯»å–è¶…æ—¶60s

# 2. æ£€æŸ¥ç›®æ ‡ç³»ç»Ÿ
# - æ•°æ®åº“è¿æ¥æ± 
# - ç¼“å­˜é…ç½®
# - æœåŠ¡å™¨èµ„æº
# - ç½‘ç»œå¸¦å®½

# 3. åˆ†æç³»ç»Ÿç“¶é¢ˆ
def analyze_system_bottlenecks():
    """åˆ†æç³»ç»Ÿç“¶é¢ˆ"""
    import psutil

    # CPUä½¿ç”¨ç‡
    cpu_percent = psutil.cpu_percent(interval=1)
    print(f"CPU Usage: {cpu_percent}%")

    # å†…å­˜ä½¿ç”¨ç‡
    memory = psutil.virtual_memory()
    print(f"Memory Usage: {memory.percent}%")

    # ç£ç›˜IO
    disk_io = psutil.disk_io_counters()
    print(f"Disk Read: {disk_io.read_bytes / 1024 / 1024:.2f}MB")
    print(f"Disk Write: {disk_io.write_bytes / 1024 / 1024:.2f}MB")

    # ç½‘ç»œIO
    net_io = psutil.net_io_counters()
    print(f"Network Sent: {net_io.bytes_sent / 1024 / 1024:.2f}MB")
    print(f"Network Recv: {net_io.bytes_recv / 1024 / 1024:.2f}MB")
```

#### é—®é¢˜ï¼šé”™è¯¯ç‡è¿‡é«˜

**ç—‡çŠ¶**
```
Error rate: 10%+
Connection errors, timeouts, 5xx responses
```

**é”™è¯¯åˆ†æå·¥å…·**
```python
# error_analyzer.py
from collections import defaultdict
import re

class ErrorAnalyzer:
    """é”™è¯¯åˆ†æå™¨"""

    def __init__(self):
        self.errors = defaultdict(list)
        self.error_patterns = {}

    def record_error(self, request_name, error_message, response_code=None):
        """è®°å½•é”™è¯¯"""
        error_info = {
            'timestamp': time.time(),
            'request_name': request_name,
            'error_message': str(error_message),
            'response_code': response_code
        }

        # åˆ†ç±»é”™è¯¯
        error_type = self.classify_error(error_message, response_code)
        self.errors[error_type].append(error_info)

    def classify_error(self, error_message, response_code):
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        error_msg = str(error_message).lower()

        if response_code:
            if 400 <= response_code < 500:
                return "client_error"
            elif 500 <= response_code < 600:
                return "server_error"

        if "timeout" in error_msg or "timed out" in error_msg:
            return "timeout_error"
        elif "connection" in error_msg:
            return "connection_error"
        elif "ssl" in error_msg or "certificate" in error_msg:
            return "ssl_error"
        elif "dns" in error_msg:
            return "dns_error"
        else:
            return "unknown_error"

    def get_error_summary(self):
        """è·å–é”™è¯¯æ‘˜è¦"""
        total_errors = sum(len(errors) for errors in self.errors.values())

        summary = {
            'total_errors': total_errors,
            'error_types': {}
        }

        for error_type, errors in self.errors.items():
            summary['error_types'][error_type] = {
                'count': len(errors),
                'percentage': len(errors) / total_errors * 100 if total_errors > 0 else 0,
                'recent_examples': [e['error_message'] for e in errors[-3:]]  # æœ€è¿‘3ä¸ªä¾‹å­
            }

        return summary

    def suggest_solutions(self):
        """å»ºè®®è§£å†³æ–¹æ¡ˆ"""
        suggestions = []

        for error_type, errors in self.errors.items():
            if len(errors) > 10:  # é”™è¯¯æ•°é‡è¾ƒå¤šæ—¶ç»™å‡ºå»ºè®®
                if error_type == "timeout_error":
                    suggestions.append("å¢åŠ è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œæ£€æŸ¥ç½‘ç»œå»¶è¿Ÿ")
                elif error_type == "connection_error":
                    suggestions.append("æ£€æŸ¥è¿æ¥æ± é…ç½®ï¼Œå¢åŠ æœ€å¤§è¿æ¥æ•°")
                elif error_type == "server_error":
                    suggestions.append("æ£€æŸ¥ç›®æ ‡æœåŠ¡å™¨çŠ¶æ€ï¼ŒæŸ¥çœ‹æœåŠ¡å™¨æ—¥å¿—")
                elif error_type == "client_error":
                    suggestions.append("æ£€æŸ¥è¯·æ±‚å‚æ•°å’Œè®¤è¯ä¿¡æ¯")
                elif error_type == "ssl_error":
                    suggestions.append("æ£€æŸ¥SSLè¯ä¹¦é…ç½®å’Œæœ‰æ•ˆæ€§")

        return suggestions

# é›†æˆé”™è¯¯åˆ†æ
error_analyzer = ErrorAnalyzer()

@events.request.add_listener
def on_request(request_type, name, response_time, response_length, exception, context, **kwargs):
    if exception:
        response_code = getattr(exception, 'response', {}).get('status_code')
        error_analyzer.record_error(name, exception, response_code)

@events.test_stop.add_listener
def on_test_stop(environment, **kwargs):
    summary = error_analyzer.get_error_summary()
    suggestions = error_analyzer.suggest_solutions()

    print("\n=== Error Analysis ===")
    print(f"Total Errors: {summary['total_errors']}")

    for error_type, info in summary['error_types'].items():
        print(f"\n{error_type}: {info['count']} ({info['percentage']:.2f}%)")
        for example in info['recent_examples']:
            print(f"  - {example}")

    if suggestions:
        print("\n=== Suggested Solutions ===")
        for suggestion in suggestions:
            print(f"- {suggestion}")
```

### 3. èµ„æºé—®é¢˜

#### é—®é¢˜ï¼šå†…å­˜ä½¿ç”¨è¿‡é«˜

**ç—‡çŠ¶**
```
Memory usage: 90%+
OOMKilled containers
Slow garbage collection
```

**å†…å­˜ç›‘æ§å·¥å…·**
```python
# memory_monitor.py
import psutil
import gc
import tracemalloc
from locust import events

class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""

    def __init__(self):
        self.memory_snapshots = []
        self.gc_stats = []
        tracemalloc.start()

    def take_snapshot(self):
        """è·å–å†…å­˜å¿«ç…§"""
        # ç³»ç»Ÿå†…å­˜
        memory = psutil.virtual_memory()

        # è¿›ç¨‹å†…å­˜
        process = psutil.Process()
        process_memory = process.memory_info()

        # Pythonå†…å­˜
        snapshot = tracemalloc.take_snapshot()
        top_stats = snapshot.statistics('lineno')

        snapshot_data = {
            'timestamp': time.time(),
            'system_memory_percent': memory.percent,
            'process_memory_mb': process_memory.rss / 1024 / 1024,
            'python_memory_mb': sum(stat.size for stat in top_stats) / 1024 / 1024,
            'top_memory_lines': [(stat.traceback.format()[-1], stat.size / 1024 / 1024)
                               for stat in top_stats[:5]]
        }

        self.memory_snapshots.append(snapshot_data)
        return snapshot_data

    def force_gc(self):
        """å¼ºåˆ¶åƒåœ¾å›æ”¶"""
        before = self.get_memory_usage()

        # æ‰§è¡Œåƒåœ¾å›æ”¶
        collected = gc.collect()

        after = self.get_memory_usage()

        gc_info = {
            'timestamp': time.time(),
            'objects_collected': collected,
            'memory_before_mb': before,
            'memory_after_mb': after,
            'memory_freed_mb': before - after
        }

        self.gc_stats.append(gc_info)
        return gc_info

    def get_memory_usage(self):
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024

    def analyze_memory_leaks(self):
        """åˆ†æå†…å­˜æ³„æ¼"""
        if len(self.memory_snapshots) < 2:
            return "Insufficient data for leak analysis"

        # æ£€æŸ¥å†…å­˜å¢é•¿è¶‹åŠ¿
        memory_growth = []
        for i in range(1, len(self.memory_snapshots)):
            current = self.memory_snapshots[i]['process_memory_mb']
            previous = self.memory_snapshots[i-1]['process_memory_mb']
            growth = current - previous
            memory_growth.append(growth)

        avg_growth = sum(memory_growth) / len(memory_growth)

        if avg_growth > 10:  # å¹³å‡å¢é•¿è¶…è¿‡10MB
            return f"Potential memory leak detected. Average growth: {avg_growth:.2f}MB per snapshot"
        else:
            return "No significant memory leak detected"

# ä½¿ç”¨å†…å­˜ç›‘æ§
memory_monitor = MemoryMonitor()

@events.test_start.add_listener
def on_test_start(environment, **kwargs):
    memory_monitor.take_snapshot()

@events.test_stop.add_listener
def on_test_stop(environment, **kwargs):
    final_snapshot = memory_monitor.take_snapshot()
    leak_analysis = memory_monitor.analyze_memory_leaks()

    print("\n=== Memory Analysis ===")
    print(f"Final Memory Usage: {final_snapshot['process_memory_mb']:.2f}MB")
    print(f"System Memory Usage: {final_snapshot['system_memory_percent']:.2f}%")
    print(f"Leak Analysis: {leak_analysis}")

    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc_info = memory_monitor.force_gc()
    print(f"GC Freed: {gc_info['memory_freed_mb']:.2f}MB")
```

**è§£å†³æ–¹æ¡ˆ**
```python
# 1. ä¼˜åŒ–å†…å­˜ä½¿ç”¨
class MemoryOptimizedUser(HttpUser):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.request_count = 0

    @task
    def memory_efficient_request(self):
        # ä½¿ç”¨æµå¼å¤„ç†å¤§å“åº”
        with self.client.get("/large-data", stream=True) as response:
            for chunk in response.iter_content(chunk_size=8192):
                # å¤„ç†æ•°æ®å—
                pass

        self.request_count += 1

        # å®šæœŸæ¸…ç†
        if self.request_count % 100 == 0:
            gc.collect()

# 2. é…ç½®åƒåœ¾å›æ”¶
import gc

# è°ƒæ•´åƒåœ¾å›æ”¶é˜ˆå€¼
gc.set_threshold(700, 10, 10)

# 3. é™åˆ¶å®¹å™¨å†…å­˜
# åœ¨docker-compose.ymlä¸­
services:
  locust-worker:
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
```

## ğŸ› ï¸ è°ƒè¯•å·¥å…·

### 1. å®æ—¶ç›‘æ§è„šæœ¬

```bash
#!/bin/bash
# monitor.sh

echo "Locust Real-time Monitor"
echo "======================="

while true; do
    clear
    echo "$(date)"
    echo "======================="

    # æœåŠ¡çŠ¶æ€
    echo "Service Status:"
    docker-compose ps
    echo ""

    # èµ„æºä½¿ç”¨
    echo "Resource Usage:"
    docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}"
    echo ""

    # æœ€æ–°æ—¥å¿—
    echo "Recent Logs:"
    docker-compose logs --tail=5 locust-master
    echo ""

    sleep 10
done
```

### 2. æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
# benchmark.py
import time
import statistics
from locust import HttpUser, task, between

class BenchmarkUser(HttpUser):
    """åŸºå‡†æµ‹è¯•ç”¨æˆ·"""

    wait_time = between(1, 2)

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.response_times = []

    @task
    def benchmark_request(self):
        """åŸºå‡†æµ‹è¯•è¯·æ±‚"""
        start_time = time.time()

        try:
            response = self.client.get("/api/health")
            response_time = (time.time() - start_time) * 1000

            self.response_times.append(response_time)

            # æ¯100ä¸ªè¯·æ±‚è¾“å‡ºç»Ÿè®¡
            if len(self.response_times) % 100 == 0:
                self.print_stats()

        except Exception as e:
            print(f"Request failed: {e}")

    def print_stats(self):
        """è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
        if self.response_times:
            avg = statistics.mean(self.response_times)
            median = statistics.median(self.response_times)
            p95 = self.percentile(self.response_times, 95)

            print(f"Stats (last {len(self.response_times)} requests):")
            print(f"  Avg: {avg:.2f}ms, Median: {median:.2f}ms, P95: {p95:.2f}ms")

    def percentile(self, data, p):
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        sorted_data = sorted(data)
        index = int(len(sorted_data) * p / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]
```

## ğŸ“š æ•…éšœé¢„é˜²

### 1. å¥åº·æ£€æŸ¥

```python
# health_check.py
import requests
import time
from typing import Dict, List

class HealthChecker:
    """å¥åº·æ£€æŸ¥å™¨"""

    def __init__(self, endpoints: List[str]):
        self.endpoints = endpoints
        self.health_history = {}

    def check_endpoint(self, endpoint: str) -> Dict:
        """æ£€æŸ¥å•ä¸ªç«¯ç‚¹"""
        try:
            start_time = time.time()
            response = requests.get(endpoint, timeout=10)
            response_time = (time.time() - start_time) * 1000

            return {
                'endpoint': endpoint,
                'status': 'healthy' if response.status_code == 200 else 'unhealthy',
                'status_code': response.status_code,
                'response_time': response_time,
                'timestamp': time.time()
            }
        except Exception as e:
            return {
                'endpoint': endpoint,
                'status': 'error',
                'error': str(e),
                'timestamp': time.time()
            }

    def check_all_endpoints(self) -> List[Dict]:
        """æ£€æŸ¥æ‰€æœ‰ç«¯ç‚¹"""
        results = []
        for endpoint in self.endpoints:
            result = self.check_endpoint(endpoint)
            results.append(result)

            # è®°å½•å†å²
            if endpoint not in self.health_history:
                self.health_history[endpoint] = []
            self.health_history[endpoint].append(result)

            # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
            if len(self.health_history[endpoint]) > 100:
                self.health_history[endpoint].pop(0)

        return results

    def get_health_summary(self) -> Dict:
        """è·å–å¥åº·çŠ¶å†µæ‘˜è¦"""
        summary = {}

        for endpoint, history in self.health_history.items():
            if history:
                healthy_count = sum(1 for h in history if h.get('status') == 'healthy')
                total_count = len(history)
                uptime_percentage = (healthy_count / total_count) * 100

                recent_response_times = [h.get('response_time', 0) for h in history[-10:]
                                       if h.get('response_time')]
                avg_response_time = sum(recent_response_times) / len(recent_response_times) if recent_response_times else 0

                summary[endpoint] = {
                    'uptime_percentage': uptime_percentage,
                    'avg_response_time': avg_response_time,
                    'total_checks': total_count,
                    'last_status': history[-1].get('status')
                }

        return summary

# ä½¿ç”¨å¥åº·æ£€æŸ¥
health_checker = HealthChecker([
    "http://localhost:8089/",
    "http://localhost:8089/stats/requests",
    "https://target-api.com/health"
])

def run_health_checks():
    """è¿è¡Œå¥åº·æ£€æŸ¥"""
    results = health_checker.check_all_endpoints()

    print("Health Check Results:")
    for result in results:
        status_icon = "âœ…" if result['status'] == 'healthy' else "âŒ"
        print(f"{status_icon} {result['endpoint']}: {result['status']}")

        if 'response_time' in result:
            print(f"   Response time: {result['response_time']:.2f}ms")
        if 'error' in result:
            print(f"   Error: {result['error']}")

    # æ˜¾ç¤ºæ‘˜è¦
    summary = health_checker.get_health_summary()
    print("\nHealth Summary:")
    for endpoint, stats in summary.items():
        print(f"{endpoint}: {stats['uptime_percentage']:.2f}% uptime, {stats['avg_response_time']:.2f}ms avg")

if __name__ == "__main__":
    run_health_checks()
```

## ğŸ‰ æ€»ç»“

æœ‰æ•ˆçš„æ•…éšœæ’é™¤éœ€è¦ï¼š

1. **ç³»ç»ŸåŒ–è¯Šæ–­**: éµå¾ªæ ‡å‡†çš„é—®é¢˜è¯Šæ–­æµç¨‹
2. **å·¥å…·æ”¯æŒ**: ä½¿ç”¨ä¸“ä¸šçš„ç›‘æ§å’Œåˆ†æå·¥å…·
3. **é¢„é˜²æªæ–½**: å»ºç«‹å¥åº·æ£€æŸ¥å’Œç›‘æ§æœºåˆ¶
4. **çŸ¥è¯†ç§¯ç´¯**: è®°å½•å’Œåˆ†äº«æ•…éšœå¤„ç†ç»éªŒ
5. **æŒç»­æ”¹è¿›**: æ ¹æ®é—®é¢˜åé¦ˆä¼˜åŒ–ç³»ç»Ÿè®¾è®¡

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [æœ€ä½³å®è·µ](best-practices.md) - é¿å…å¸¸è§é—®é¢˜çš„æœ€ä½³å®è·µ
- [ç›‘æ§é…ç½®](../configuration/monitoring-config.md) - ç›‘æ§ç³»ç»Ÿé…ç½®
- [ç”Ÿäº§ç¯å¢ƒé…ç½®](../configuration/production.md) - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- [å¸¸è§é—®é¢˜è§£ç­”](faq.md) - å¸¸è§é—®é¢˜å¿«é€Ÿè§£ç­”
