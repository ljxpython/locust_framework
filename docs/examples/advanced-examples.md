# é«˜çº§ç¤ºä¾‹

æœ¬æ–‡æ¡£æä¾›Locustæ€§èƒ½æµ‹è¯•æ¡†æ¶çš„é«˜çº§ä½¿ç”¨ç¤ºä¾‹ï¼Œæ¶µç›–å¤æ‚åœºæ™¯ã€é«˜çº§åŠŸèƒ½å’Œå®é™…é¡¹ç›®åº”ç”¨æ¡ˆä¾‹ã€‚

## ğŸ¯ ç›®æ ‡è¯»è€…

- æœ‰ä¸€å®šLocustä½¿ç”¨ç»éªŒçš„å¼€å‘è€…
- éœ€è¦å®ç°å¤æ‚æµ‹è¯•åœºæ™¯çš„æµ‹è¯•å·¥ç¨‹å¸ˆ
- å¸Œæœ›æ·±å…¥äº†è§£æ¡†æ¶é«˜çº§åŠŸèƒ½çš„ç”¨æˆ·

## ğŸ“‹ å‰ç½®æ¡ä»¶

- å·²å®Œæˆ[åŸºç¡€ç¤ºä¾‹](basic-examples.md)çš„å­¦ä¹ 
- ç†Ÿæ‚‰Pythonç¼–ç¨‹å’Œé¢å‘å¯¹è±¡æ¦‚å¿µ
- äº†è§£HTTPåè®®å’ŒWebæœåŠ¡æ¶æ„
- å…·å¤‡æ€§èƒ½æµ‹è¯•åŸºç¡€çŸ¥è¯†

## ğŸš€ é«˜çº§è´Ÿè½½æ¨¡å¼

### 1. è‡ªé€‚åº”è´Ÿè½½æ¨¡å¼

```python
# locustfiles/adaptive_load_shape.py
from locust import LoadTestShape
import time
import requests

class AdaptiveLoadShape(LoadTestShape):
    """
    è‡ªé€‚åº”è´Ÿè½½æ¨¡å¼ - æ ¹æ®ç³»ç»Ÿå“åº”æ—¶é—´åŠ¨æ€è°ƒæ•´è´Ÿè½½
    """

    def __init__(self):
        super().__init__()
        self.start_time = time.time()
        self.current_users = 10
        self.max_users = 1000
        self.min_users = 5
        self.target_response_time = 500  # ç›®æ ‡å“åº”æ—¶é—´(ms)
        self.adjustment_interval = 30  # è°ƒæ•´é—´éš”(ç§’)
        self.last_adjustment = 0

    def tick(self):
        run_time = time.time() - self.start_time

        # æ¯30ç§’è°ƒæ•´ä¸€æ¬¡è´Ÿè½½
        if run_time - self.last_adjustment >= self.adjustment_interval:
            self.adjust_load()
            self.last_adjustment = run_time

        if run_time < 3600:  # è¿è¡Œ1å°æ—¶
            return (self.current_users, 5)
        else:
            return None

    def adjust_load(self):
        """æ ¹æ®å½“å‰æ€§èƒ½æŒ‡æ ‡è°ƒæ•´è´Ÿè½½"""
        try:
            # è·å–å½“å‰ç»Ÿè®¡ä¿¡æ¯
            stats = self.runner.stats.total

            if stats.num_requests > 0:
                avg_response_time = stats.avg_response_time
                error_rate = stats.num_failures / stats.num_requests

                # æ ¹æ®å“åº”æ—¶é—´è°ƒæ•´
                if avg_response_time > self.target_response_time * 1.2:
                    # å“åº”æ—¶é—´è¿‡é«˜ï¼Œå‡å°‘ç”¨æˆ·æ•°
                    self.current_users = max(
                        self.min_users,
                        int(self.current_users * 0.9)
                    )
                    print(f"High response time ({avg_response_time:.2f}ms), reducing users to {self.current_users}")

                elif avg_response_time < self.target_response_time * 0.8 and error_rate < 0.01:
                    # å“åº”æ—¶é—´è‰¯å¥½ä¸”é”™è¯¯ç‡ä½ï¼Œå¢åŠ ç”¨æˆ·æ•°
                    self.current_users = min(
                        self.max_users,
                        int(self.current_users * 1.1)
                    )
                    print(f"Good performance, increasing users to {self.current_users}")

        except Exception as e:
            print(f"Error adjusting load: {e}")

class AdaptiveUser(HttpUser):
    """è‡ªé€‚åº”ç”¨æˆ·è¡Œä¸º"""

    wait_time = between(1, 3)

    def on_start(self):
        """ç”¨æˆ·å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
        self.login()

    def login(self):
        """ç”¨æˆ·ç™»å½•"""
        response = self.client.post("/auth/login", json={
            "username": f"user_{random.randint(1, 10000)}",
            "password": "password123"
        })

        if response.status_code == 200:
            self.token = response.json().get("token")
            self.client.headers.update({
                "Authorization": f"Bearer {self.token}"
            })

    @task(3)
    def browse_products(self):
        """æµè§ˆäº§å“"""
        # è·å–äº§å“åˆ—è¡¨
        response = self.client.get("/api/products", params={
            "page": random.randint(1, 10),
            "limit": 20
        })

        if response.status_code == 200:
            products = response.json().get("products", [])

            # éšæœºæŸ¥çœ‹äº§å“è¯¦æƒ…
            if products:
                product = random.choice(products)
                self.client.get(f"/api/products/{product['id']}")

    @task(2)
    def search_products(self):
        """æœç´¢äº§å“"""
        keywords = ["laptop", "phone", "tablet", "camera", "headphones"]
        keyword = random.choice(keywords)

        self.client.get("/api/search", params={
            "q": keyword,
            "sort": random.choice(["price", "rating", "popularity"])
        })

    @task(1)
    def add_to_cart(self):
        """æ·»åŠ åˆ°è´­ç‰©è½¦"""
        # å…ˆè·å–ä¸€ä¸ªäº§å“
        response = self.client.get("/api/products", params={"limit": 1})

        if response.status_code == 200:
            products = response.json().get("products", [])
            if products:
                product = products[0]
                self.client.post("/api/cart/add", json={
                    "product_id": product["id"],
                    "quantity": random.randint(1, 3)
                })
```

### 2. å¤šé˜¶æ®µå‹åŠ›æµ‹è¯•

```python
# locustfiles/multi_stage_stress.py
from locust import LoadTestShape, HttpUser, task, between
import time

class MultiStageStressShape(LoadTestShape):
    """
    å¤šé˜¶æ®µå‹åŠ›æµ‹è¯•æ¨¡å¼
    æ¨¡æ‹ŸçœŸå®ä¸šåŠ¡åœºæ™¯çš„è´Ÿè½½å˜åŒ–
    """

    stages = [
        # é˜¶æ®µ1: é¢„çƒ­é˜¶æ®µ
        {"duration": 300, "users": 50, "spawn_rate": 2, "name": "warmup"},

        # é˜¶æ®µ2: æ­£å¸¸è´Ÿè½½
        {"duration": 600, "users": 200, "spawn_rate": 5, "name": "normal_load"},

        # é˜¶æ®µ3: é«˜å³°è´Ÿè½½
        {"duration": 900, "users": 500, "spawn_rate": 10, "name": "peak_load"},

        # é˜¶æ®µ4: æé™å‹åŠ›
        {"duration": 1200, "users": 1000, "spawn_rate": 20, "name": "stress_test"},

        # é˜¶æ®µ5: æ¢å¤æµ‹è¯•
        {"duration": 1500, "users": 100, "spawn_rate": -30, "name": "recovery"},

        # é˜¶æ®µ6: ç¨³å®šæ€§æµ‹è¯•
        {"duration": 2100, "users": 300, "spawn_rate": 5, "name": "stability"},
    ]

    def tick(self):
        run_time = self.get_run_time()

        for stage in self.stages:
            if run_time < stage["duration"]:
                # è®°å½•å½“å‰é˜¶æ®µä¿¡æ¯
                if not hasattr(self, 'current_stage') or self.current_stage != stage["name"]:
                    self.current_stage = stage["name"]
                    print(f"Entering stage: {stage['name']} - Users: {stage['users']}, Duration: {stage['duration']}s")

                return (stage["users"], stage["spawn_rate"])

        return None

class StressTestUser(HttpUser):
    """å‹åŠ›æµ‹è¯•ç”¨æˆ·"""

    wait_time = between(0.5, 2)

    def on_start(self):
        """ç”¨æˆ·åˆå§‹åŒ–"""
        self.user_id = random.randint(1, 100000)
        self.session_data = {}
        self.login()

    def login(self):
        """ç”¨æˆ·ç™»å½•"""
        with self.client.post("/auth/login", json={
            "username": f"stress_user_{self.user_id}",
            "password": "stress_test_password"
        }, catch_response=True) as response:
            if response.status_code == 200:
                self.session_data = response.json()
                self.client.headers.update({
                    "Authorization": f"Bearer {self.session_data.get('token')}"
                })
            else:
                response.failure(f"Login failed: {response.status_code}")

    @task(5)
    def high_frequency_api_calls(self):
        """é«˜é¢‘APIè°ƒç”¨"""
        endpoints = [
            "/api/user/profile",
            "/api/notifications",
            "/api/dashboard/stats",
            "/api/recent/activities"
        ]

        for endpoint in random.sample(endpoints, random.randint(1, 3)):
            with self.client.get(endpoint, catch_response=True) as response:
                if response.status_code != 200:
                    response.failure(f"API call failed: {endpoint}")

    @task(3)
    def data_intensive_operations(self):
        """æ•°æ®å¯†é›†å‹æ“ä½œ"""
        # å¤§æ•°æ®é‡æŸ¥è¯¢
        self.client.get("/api/reports/large-dataset", params={
            "start_date": "2023-01-01",
            "end_date": "2023-12-31",
            "include_details": True
        })

        # æ‰¹é‡æ•°æ®å¤„ç†
        batch_data = [
            {"id": i, "value": f"data_{i}", "timestamp": time.time()}
            for i in range(100)
        ]

        self.client.post("/api/batch/process", json={
            "data": batch_data,
            "operation": "bulk_insert"
        })

    @task(2)
    def concurrent_operations(self):
        """å¹¶å‘æ“ä½œæµ‹è¯•"""
        import threading

        def concurrent_request(endpoint, data=None):
            if data:
                self.client.post(endpoint, json=data)
            else:
                self.client.get(endpoint)

        # åˆ›å»ºå¤šä¸ªå¹¶å‘è¯·æ±‚
        threads = []
        operations = [
            ("/api/concurrent/read", None),
            ("/api/concurrent/write", {"data": f"concurrent_data_{time.time()}"}),
            ("/api/concurrent/update", {"id": self.user_id, "status": "active"}),
        ]

        for endpoint, data in operations:
            thread = threading.Thread(target=concurrent_request, args=(endpoint, data))
            threads.append(thread)
            thread.start()

        # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
        for thread in threads:
            thread.join()

    @task(1)
    def memory_intensive_operations(self):
        """å†…å­˜å¯†é›†å‹æ“ä½œ"""
        # è¯·æ±‚å¤§æ–‡ä»¶
        self.client.get("/api/files/large-download", stream=True)

        # ä¸Šä¼ å¤§æ–‡ä»¶
        large_data = "x" * (1024 * 1024)  # 1MBæ•°æ®
        self.client.post("/api/files/upload", files={
            "file": ("large_file.txt", large_data, "text/plain")
        })
```

## ğŸ”§ é«˜çº§æ’ä»¶å¼€å‘

### 1. è‡ªå®šä¹‰æ€§èƒ½åˆ†ææ’ä»¶

```python
# src/plugins/advanced_analysis.py
from src.core.plugin_base import AnalysisPlugin
import numpy as np
import pandas as pd
from scipy import stats
from typing import Dict, List, Any
import matplotlib.pyplot as plt
import seaborn as sns

class AdvancedAnalysisPlugin(AnalysisPlugin):
    """é«˜çº§æ€§èƒ½åˆ†ææ’ä»¶"""

    def __init__(self):
        super().__init__()
        self.name = "advanced_analysis"
        self.version = "1.0.0"
        self.description = "Advanced statistical analysis and machine learning insights"

    def analyze(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œé«˜çº§åˆ†æ"""
        results = {
            "statistical_analysis": self._statistical_analysis(data),
            "trend_analysis": self._trend_analysis(data),
            "anomaly_detection": self._anomaly_detection(data),
            "performance_prediction": self._performance_prediction(data),
            "bottleneck_analysis": self._bottleneck_analysis(data)
        }

        return results

    def _statistical_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ç»Ÿè®¡åˆ†æ"""
        response_times = data.get("response_times", [])

        if not response_times:
            return {"error": "No response time data available"}

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        rt_array = np.array(response_times)

        # åŸºç¡€ç»Ÿè®¡
        basic_stats = {
            "count": len(rt_array),
            "mean": np.mean(rt_array),
            "median": np.median(rt_array),
            "std": np.std(rt_array),
            "variance": np.var(rt_array),
            "min": np.min(rt_array),
            "max": np.max(rt_array),
            "range": np.max(rt_array) - np.min(rt_array)
        }

        # ç™¾åˆ†ä½æ•°
        percentiles = {
            f"p{p}": np.percentile(rt_array, p)
            for p in [50, 75, 90, 95, 99, 99.9]
        }

        # åˆ†å¸ƒæ£€éªŒ
        distribution_tests = {
            "normality_test": stats.normaltest(rt_array),
            "skewness": stats.skew(rt_array),
            "kurtosis": stats.kurtosis(rt_array)
        }

        # ç½®ä¿¡åŒºé—´
        confidence_interval = stats.t.interval(
            0.95, len(rt_array)-1,
            loc=np.mean(rt_array),
            scale=stats.sem(rt_array)
        )

        return {
            "basic_stats": basic_stats,
            "percentiles": percentiles,
            "distribution_tests": distribution_tests,
            "confidence_interval_95": confidence_interval
        }

    def _trend_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """è¶‹åŠ¿åˆ†æ"""
        timestamps = data.get("timestamps", [])
        response_times = data.get("response_times", [])

        if len(timestamps) != len(response_times) or len(timestamps) < 10:
            return {"error": "Insufficient data for trend analysis"}

        # åˆ›å»ºæ—¶é—´åºåˆ—
        df = pd.DataFrame({
            "timestamp": pd.to_datetime(timestamps),
            "response_time": response_times
        })
        df.set_index("timestamp", inplace=True)

        # ç§»åŠ¨å¹³å‡
        df["ma_5"] = df["response_time"].rolling(window=5).mean()
        df["ma_20"] = df["response_time"].rolling(window=20).mean()

        # è¶‹åŠ¿æ£€æµ‹
        x = np.arange(len(df))
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, df["response_time"])

        # å­£èŠ‚æ€§åˆ†æï¼ˆå¦‚æœæ•°æ®è¶³å¤Ÿï¼‰
        seasonal_analysis = {}
        if len(df) >= 100:
            # ç®€å•çš„å‘¨æœŸæ€§æ£€æµ‹
            autocorr = [df["response_time"].autocorr(lag=i) for i in range(1, 25)]
            max_autocorr_lag = np.argmax(autocorr) + 1
            seasonal_analysis = {
                "potential_period": max_autocorr_lag,
                "autocorrelation": max(autocorr)
            }

        return {
            "trend": {
                "slope": slope,
                "r_squared": r_value**2,
                "p_value": p_value,
                "trend_direction": "increasing" if slope > 0 else "decreasing"
            },
            "moving_averages": {
                "ma_5_current": df["ma_5"].iloc[-1] if not df["ma_5"].isna().iloc[-1] else None,
                "ma_20_current": df["ma_20"].iloc[-1] if not df["ma_20"].isna().iloc[-1] else None
            },
            "seasonal_analysis": seasonal_analysis
        }

    def _anomaly_detection(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¼‚å¸¸æ£€æµ‹"""
        response_times = np.array(data.get("response_times", []))

        if len(response_times) < 20:
            return {"error": "Insufficient data for anomaly detection"}

        # Z-scoreæ–¹æ³•
        z_scores = np.abs(stats.zscore(response_times))
        z_anomalies = np.where(z_scores > 3)[0]

        # IQRæ–¹æ³•
        q1, q3 = np.percentile(response_times, [25, 75])
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        iqr_anomalies = np.where(
            (response_times < lower_bound) | (response_times > upper_bound)
        )[0]

        # ç§»åŠ¨çª—å£å¼‚å¸¸æ£€æµ‹
        window_size = min(20, len(response_times) // 4)
        moving_anomalies = []

        for i in range(window_size, len(response_times)):
            window = response_times[i-window_size:i]
            current = response_times[i]

            if abs(current - np.mean(window)) > 2 * np.std(window):
                moving_anomalies.append(i)

        return {
            "z_score_anomalies": {
                "count": len(z_anomalies),
                "indices": z_anomalies.tolist(),
                "percentage": len(z_anomalies) / len(response_times) * 100
            },
            "iqr_anomalies": {
                "count": len(iqr_anomalies),
                "indices": iqr_anomalies.tolist(),
                "percentage": len(iqr_anomalies) / len(response_times) * 100
            },
            "moving_window_anomalies": {
                "count": len(moving_anomalies),
                "indices": moving_anomalies,
                "percentage": len(moving_anomalies) / len(response_times) * 100
            }
        }

    def _performance_prediction(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ€§èƒ½é¢„æµ‹"""
        user_counts = data.get("user_counts", [])
        response_times = data.get("response_times", [])

        if len(user_counts) != len(response_times) or len(user_counts) < 10:
            return {"error": "Insufficient data for performance prediction"}

        # çº¿æ€§å›å½’é¢„æµ‹
        slope, intercept, r_value, p_value, std_err = stats.linregress(user_counts, response_times)

        # é¢„æµ‹ä¸åŒç”¨æˆ·æ•°ä¸‹çš„å“åº”æ—¶é—´
        prediction_points = [100, 200, 500, 1000, 2000]
        predictions = {}

        for users in prediction_points:
            predicted_rt = slope * users + intercept
            predictions[f"users_{users}"] = {
                "predicted_response_time": predicted_rt,
                "confidence": r_value**2
            }

        # å®¹é‡é¢„æµ‹ï¼ˆå“åº”æ—¶é—´é˜ˆå€¼ï¼‰
        rt_thresholds = [500, 1000, 2000, 5000]  # ms
        capacity_predictions = {}

        for threshold in rt_thresholds:
            if slope > 0:
                predicted_users = (threshold - intercept) / slope
                capacity_predictions[f"rt_{threshold}ms"] = max(0, predicted_users)

        return {
            "regression_model": {
                "slope": slope,
                "intercept": intercept,
                "r_squared": r_value**2,
                "p_value": p_value,
                "standard_error": std_err
            },
            "response_time_predictions": predictions,
            "capacity_predictions": capacity_predictions
        }

    def _bottleneck_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ç“¶é¢ˆåˆ†æ"""
        endpoint_stats = data.get("endpoint_stats", {})

        if not endpoint_stats:
            return {"error": "No endpoint statistics available"}

        # åˆ†æå„ç«¯ç‚¹æ€§èƒ½
        bottlenecks = []

        for endpoint, stats in endpoint_stats.items():
            avg_rt = stats.get("avg_response_time", 0)
            error_rate = stats.get("error_rate", 0)
            request_count = stats.get("request_count", 0)

            # è®¡ç®—ç“¶é¢ˆåˆ†æ•°
            bottleneck_score = (
                (avg_rt / 1000) * 0.4 +  # å“åº”æ—¶é—´æƒé‡40%
                error_rate * 0.4 +        # é”™è¯¯ç‡æƒé‡40%
                (request_count / 1000) * 0.2  # è¯·æ±‚é‡æƒé‡20%
            )

            bottlenecks.append({
                "endpoint": endpoint,
                "bottleneck_score": bottleneck_score,
                "avg_response_time": avg_rt,
                "error_rate": error_rate,
                "request_count": request_count
            })

        # æŒ‰ç“¶é¢ˆåˆ†æ•°æ’åº
        bottlenecks.sort(key=lambda x: x["bottleneck_score"], reverse=True)

        # è¯†åˆ«ä¸»è¦ç“¶é¢ˆ
        top_bottlenecks = bottlenecks[:5]

        # ç“¶é¢ˆåˆ†ç±»
        performance_bottlenecks = [b for b in bottlenecks if b["avg_response_time"] > 1000]
        reliability_bottlenecks = [b for b in bottlenecks if b["error_rate"] > 0.05]

        return {
            "top_bottlenecks": top_bottlenecks,
            "performance_bottlenecks": performance_bottlenecks,
            "reliability_bottlenecks": reliability_bottlenecks,
            "total_endpoints_analyzed": len(endpoint_stats)
        }

    def generate_report(self, analysis_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆé«˜çº§åˆ†ææŠ¥å‘Š"""
        report = []
        report.append("# é«˜çº§æ€§èƒ½åˆ†ææŠ¥å‘Š\n")

        # ç»Ÿè®¡åˆ†æéƒ¨åˆ†
        if "statistical_analysis" in analysis_results:
            stats = analysis_results["statistical_analysis"]
            if "basic_stats" in stats:
                basic = stats["basic_stats"]
                report.append("## ç»Ÿè®¡åˆ†æ")
                report.append(f"- å¹³å‡å“åº”æ—¶é—´: {basic['mean']:.2f}ms")
                report.append(f"- ä¸­ä½æ•°å“åº”æ—¶é—´: {basic['median']:.2f}ms")
                report.append(f"- æ ‡å‡†å·®: {basic['std']:.2f}ms")
                report.append(f"- 95%ç½®ä¿¡åŒºé—´: {stats.get('confidence_interval_95', 'N/A')}")
                report.append("")

        # è¶‹åŠ¿åˆ†æéƒ¨åˆ†
        if "trend_analysis" in analysis_results:
            trend = analysis_results["trend_analysis"]
            if "trend" in trend:
                t = trend["trend"]
                report.append("## è¶‹åŠ¿åˆ†æ")
                report.append(f"- è¶‹åŠ¿æ–¹å‘: {t['trend_direction']}")
                report.append(f"- RÂ²å€¼: {t['r_squared']:.4f}")
                report.append(f"- ç»Ÿè®¡æ˜¾è‘—æ€§: {'æ˜¾è‘—' if t['p_value'] < 0.05 else 'ä¸æ˜¾è‘—'}")
                report.append("")

        # å¼‚å¸¸æ£€æµ‹éƒ¨åˆ†
        if "anomaly_detection" in analysis_results:
            anomaly = analysis_results["anomaly_detection"]
            report.append("## å¼‚å¸¸æ£€æµ‹")
            if "z_score_anomalies" in anomaly:
                z_anom = anomaly["z_score_anomalies"]
                report.append(f"- Z-scoreå¼‚å¸¸: {z_anom['count']}ä¸ª ({z_anom['percentage']:.2f}%)")
            if "iqr_anomalies" in anomaly:
                iqr_anom = anomaly["iqr_anomalies"]
                report.append(f"- IQRå¼‚å¸¸: {iqr_anom['count']}ä¸ª ({iqr_anom['percentage']:.2f}%)")
            report.append("")

        # ç“¶é¢ˆåˆ†æéƒ¨åˆ†
        if "bottleneck_analysis" in analysis_results:
            bottleneck = analysis_results["bottleneck_analysis"]
            if "top_bottlenecks" in bottleneck:
                report.append("## ç“¶é¢ˆåˆ†æ")
                report.append("### ä¸»è¦ç“¶é¢ˆç«¯ç‚¹:")
                for i, b in enumerate(bottleneck["top_bottlenecks"][:3], 1):
                    report.append(f"{i}. {b['endpoint']} (åˆ†æ•°: {b['bottleneck_score']:.2f})")
                    report.append(f"   - å¹³å‡å“åº”æ—¶é—´: {b['avg_response_time']:.2f}ms")
                    report.append(f"   - é”™è¯¯ç‡: {b['error_rate']:.2%}")
                report.append("")

        return "\n".join(report)
```
