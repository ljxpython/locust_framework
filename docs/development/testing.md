# æµ‹è¯•æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»æ¡†æ¶çš„æµ‹è¯•ç­–ç•¥ã€æµ‹è¯•å·¥å…·ä½¿ç”¨ã€æµ‹è¯•æœ€ä½³å®è·µç­‰å†…å®¹ã€‚

## ğŸ¯ æµ‹è¯•ç­–ç•¥

### æµ‹è¯•é‡‘å­—å¡”

```
    /\
   /  \     E2E Tests (ç«¯åˆ°ç«¯æµ‹è¯•)
  /____\    Integration Tests (é›†æˆæµ‹è¯•)
 /______\   Unit Tests (å•å…ƒæµ‹è¯•)
/__________\
```

- **å•å…ƒæµ‹è¯• (70%)**: æµ‹è¯•å•ä¸ªå‡½æ•°ã€æ–¹æ³•ã€ç±»
- **é›†æˆæµ‹è¯• (20%)**: æµ‹è¯•æ¨¡å—é—´äº¤äº’
- **ç«¯åˆ°ç«¯æµ‹è¯• (10%)**: æµ‹è¯•å®Œæ•´ç”¨æˆ·åœºæ™¯

### æµ‹è¯•åˆ†ç±»

```python
# æµ‹è¯•æ ‡è®°åˆ†ç±»
pytest.mark.unit        # å•å…ƒæµ‹è¯•
pytest.mark.integration # é›†æˆæµ‹è¯•
pytest.mark.e2e         # ç«¯åˆ°ç«¯æµ‹è¯•
pytest.mark.slow        # æ…¢é€Ÿæµ‹è¯•
pytest.mark.smoke       # å†’çƒŸæµ‹è¯•
pytest.mark.regression  # å›å½’æµ‹è¯•
```

## ğŸ§ª å•å…ƒæµ‹è¯•

### æµ‹è¯•ç»“æ„

```python
# tests/unit/test_performance_analyzer.py
import pytest
from unittest.mock import Mock, patch, MagicMock
from src.analysis.performance_analyzer import PerformanceAnalyzer
from src.exceptions import AnalysisError

class TestPerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨å•å…ƒæµ‹è¯•"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•å‰æ‰§è¡Œ"""
        self.analyzer = PerformanceAnalyzer()
        self.sample_data = {
            'test_name': 'APIæ€§èƒ½æµ‹è¯•',
            'start_time': '2023-12-01 10:00:00',
            'end_time': '2023-12-01 10:10:00',
            'duration': 600,
            'users': 50,
            'requests': [
                {'response_time': 100, 'success': True, 'timestamp': '2023-12-01 10:01:00'},
                {'response_time': 200, 'success': True, 'timestamp': '2023-12-01 10:02:00'},
                {'response_time': 150, 'success': False, 'timestamp': '2023-12-01 10:03:00'}
            ]
        }

    def teardown_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•åæ‰§è¡Œ"""
        # æ¸…ç†èµ„æº
        pass

    @pytest.mark.unit
    def test_analyze_performance_success(self):
        """æµ‹è¯•æ€§èƒ½åˆ†ææˆåŠŸåœºæ™¯"""
        # Given
        expected_grade = 'B'

        # When
        result = self.analyzer.comprehensive_analysis(self.sample_data)

        # Then
        assert result is not None
        assert result['overall_grade'] == expected_grade
        assert 'response_time' in result
        assert 'throughput' in result
        assert 'error_analysis' in result
        assert 'recommendations' in result

    @pytest.mark.unit
    def test_analyze_empty_requests(self):
        """æµ‹è¯•ç©ºè¯·æ±‚åˆ—è¡¨"""
        # Given
        empty_data = self.sample_data.copy()
        empty_data['requests'] = []

        # When & Then
        with pytest.raises(AnalysisError, match="è¯·æ±‚æ•°æ®ä¸èƒ½ä¸ºç©º"):
            self.analyzer.comprehensive_analysis(empty_data)

    @pytest.mark.unit
    @pytest.mark.parametrize("response_times,expected_grade", [
        ([50, 80, 60, 70, 90], 'A'),      # ä¼˜ç§€
        ([100, 200, 150, 180, 120], 'B'), # è‰¯å¥½
        ([300, 400, 350, 380, 320], 'C'), # ä¸€èˆ¬
        ([800, 900, 850, 880, 820], 'D'), # è¾ƒå·®
    ])
    def test_grading_algorithm(self, response_times, expected_grade):
        """å‚æ•°åŒ–æµ‹è¯•è¯„çº§ç®—æ³•"""
        # Given
        test_data = self.sample_data.copy()
        test_data['requests'] = [
            {'response_time': rt, 'success': True, 'timestamp': '2023-12-01 10:01:00'}
            for rt in response_times
        ]

        # When
        result = self.analyzer.comprehensive_analysis(test_data)

        # Then
        assert result['overall_grade'] == expected_grade

    @pytest.mark.unit
    @patch('src.analysis.performance_analyzer.calculate_percentile')
    def test_response_time_calculation_with_mock(self, mock_percentile):
        """ä½¿ç”¨Mockæµ‹è¯•å“åº”æ—¶é—´è®¡ç®—"""
        # Given
        mock_percentile.side_effect = [50.0, 95.0, 99.0]  # P50, P95, P99

        # When
        result = self.analyzer.comprehensive_analysis(self.sample_data)

        # Then
        assert mock_percentile.call_count == 3
        assert result['response_time']['p50'] == 50.0
        assert result['response_time']['p95'] == 95.0
        assert result['response_time']['p99'] == 99.0

    @pytest.mark.unit
    def test_error_rate_calculation(self):
        """æµ‹è¯•é”™è¯¯ç‡è®¡ç®—"""
        # Given
        test_data = self.sample_data.copy()
        test_data['requests'] = [
            {'response_time': 100, 'success': True, 'timestamp': '2023-12-01 10:01:00'},
            {'response_time': 200, 'success': False, 'timestamp': '2023-12-01 10:02:00'},
            {'response_time': 150, 'success': False, 'timestamp': '2023-12-01 10:03:00'},
            {'response_time': 180, 'success': True, 'timestamp': '2023-12-01 10:04:00'}
        ]

        # When
        result = self.analyzer.comprehensive_analysis(test_data)

        # Then
        expected_error_rate = 50.0  # 2/4 = 50%
        assert result['error_analysis']['error_rate'] == expected_error_rate

    @pytest.mark.unit
    def test_throughput_calculation(self):
        """æµ‹è¯•ååé‡è®¡ç®—"""
        # Given
        test_data = self.sample_data.copy()
        test_data['duration'] = 60  # 60ç§’
        test_data['requests'] = [
            {'response_time': 100, 'success': True, 'timestamp': '2023-12-01 10:01:00'}
            for _ in range(120)  # 120ä¸ªè¯·æ±‚
        ]

        # When
        result = self.analyzer.comprehensive_analysis(test_data)

        # Then
        expected_tps = 2.0  # 120è¯·æ±‚ / 60ç§’ = 2 TPS
        assert result['throughput']['requests_per_second'] == expected_tps
```

### æµ‹è¯•å·¥å…·å’ŒæŠ€å·§

```python
# ä½¿ç”¨fixtures
@pytest.fixture
def sample_analyzer():
    """åˆ†æå™¨fixture"""
    return PerformanceAnalyzer()

@pytest.fixture
def sample_test_data():
    """æµ‹è¯•æ•°æ®fixture"""
    return {
        'requests': [
            {'response_time': 100, 'success': True},
            {'response_time': 200, 'success': True}
        ],
        'duration': 60,
        'users': 10
    }

# ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
@pytest.fixture
def temp_config_file(tmp_path):
    """ä¸´æ—¶é…ç½®æ–‡ä»¶"""
    config_file = tmp_path / "test_config.toml"
    config_file.write_text("""
    [analysis]
    threshold = 1000
    """)
    return str(config_file)

# ä½¿ç”¨Mockå¯¹è±¡
def test_with_mock_dependencies():
    """ä½¿ç”¨Mockæµ‹è¯•ä¾èµ–"""
    # Given
    mock_data_provider = Mock()
    mock_data_provider.get_test_data.return_value = {'requests': []}

    analyzer = PerformanceAnalyzer(data_provider=mock_data_provider)

    # When
    result = analyzer.load_and_analyze('test_id')

    # Then
    mock_data_provider.get_test_data.assert_called_once_with('test_id')

# å¼‚å¸¸æµ‹è¯•
def test_exception_handling():
    """æµ‹è¯•å¼‚å¸¸å¤„ç†"""
    analyzer = PerformanceAnalyzer()

    with pytest.raises(ValueError, match="æ— æ•ˆçš„æµ‹è¯•æ•°æ®"):
        analyzer.comprehensive_analysis(None)

# å±æ€§æµ‹è¯•
@pytest.mark.property
@given(st.lists(st.integers(min_value=1, max_value=5000), min_size=1))
def test_response_time_analysis_property(response_times):
    """å±æ€§æµ‹è¯•ï¼šå“åº”æ—¶é—´åˆ†æ"""
    # Given
    test_data = {
        'requests': [{'response_time': rt, 'success': True} for rt in response_times],
        'duration': 60,
        'users': 10
    }

    analyzer = PerformanceAnalyzer()

    # When
    result = analyzer.comprehensive_analysis(test_data)

    # Then
    assert result['response_time']['min'] == min(response_times)
    assert result['response_time']['max'] == max(response_times)
    assert result['response_time']['avg'] == sum(response_times) / len(response_times)
```

## ğŸ”— é›†æˆæµ‹è¯•

### æ¨¡å—é—´é›†æˆæµ‹è¯•

```python
# tests/integration/test_analysis_integration.py
import pytest
from src.analysis.performance_analyzer import PerformanceAnalyzer
from src.data_manager.data_provider import DataProvider
from src.monitoring.performance_monitor import PerformanceMonitor

class TestAnalysisIntegration:
    """åˆ†ææ¨¡å—é›†æˆæµ‹è¯•"""

    @pytest.fixture
    def integrated_system(self):
        """é›†æˆç³»ç»Ÿfixture"""
        data_provider = DataProvider()
        monitor = PerformanceMonitor()
        analyzer = PerformanceAnalyzer(
            data_provider=data_provider,
            monitor=monitor
        )
        return {
            'data_provider': data_provider,
            'monitor': monitor,
            'analyzer': analyzer
        }

    @pytest.mark.integration
    def test_end_to_end_analysis_flow(self, integrated_system):
        """ç«¯åˆ°ç«¯åˆ†ææµç¨‹æµ‹è¯•"""
        # Given
        system = integrated_system
        test_data_file = "tests/fixtures/sample_test_data.json"

        # When
        # 1. åŠ è½½æµ‹è¯•æ•°æ®
        system['data_provider'].load_data_from_file(test_data_file)

        # 2. å¯åŠ¨ç›‘æ§
        system['monitor'].start_monitoring()

        # 3. æ‰§è¡Œåˆ†æ
        test_data = system['data_provider'].get_test_data('sample_test')
        result = system['analyzer'].comprehensive_analysis(test_data)

        # 4. åœæ­¢ç›‘æ§
        system['monitor'].stop_monitoring()

        # Then
        assert result is not None
        assert 'overall_grade' in result
        assert result['overall_grade'] in ['A', 'B', 'C', 'D']

    @pytest.mark.integration
    def test_data_flow_between_modules(self, integrated_system):
        """æµ‹è¯•æ¨¡å—é—´æ•°æ®æµ"""
        # Given
        system = integrated_system

        # When
        # æ¨¡æ‹Ÿæ•°æ®åœ¨æ¨¡å—é—´æµè½¬
        raw_data = {'requests': [{'response_time': 100, 'success': True}]}
        system['data_provider'].store_test_data('test_001', raw_data)

        retrieved_data = system['data_provider'].get_test_data('test_001')
        analysis_result = system['analyzer'].comprehensive_analysis(retrieved_data)

        # Then
        assert retrieved_data == raw_data
        assert analysis_result is not None
```

### æ•°æ®åº“é›†æˆæµ‹è¯•

```python
# tests/integration/test_database_integration.py
import pytest
import sqlite3
from src.data_manager.data_provider import DataProvider

class TestDatabaseIntegration:
    """æ•°æ®åº“é›†æˆæµ‹è¯•"""

    @pytest.fixture
    def test_database(self, tmp_path):
        """æµ‹è¯•æ•°æ®åº“fixture"""
        db_path = tmp_path / "test.db"
        conn = sqlite3.connect(str(db_path))

        # åˆ›å»ºæµ‹è¯•è¡¨
        conn.execute("""
            CREATE TABLE test_results (
                id INTEGER PRIMARY KEY,
                test_name TEXT,
                response_time REAL,
                success BOOLEAN,
                timestamp TEXT
            )
        """)

        # æ’å…¥æµ‹è¯•æ•°æ®
        test_data = [
            ('APIæµ‹è¯•', 100.5, True, '2023-12-01 10:00:00'),
            ('APIæµ‹è¯•', 200.3, True, '2023-12-01 10:01:00'),
            ('APIæµ‹è¯•', 150.8, False, '2023-12-01 10:02:00')
        ]

        conn.executemany(
            "INSERT INTO test_results (test_name, response_time, success, timestamp) VALUES (?, ?, ?, ?)",
            test_data
        )
        conn.commit()
        conn.close()

        return str(db_path)

    @pytest.mark.integration
    def test_load_data_from_database(self, test_database):
        """æµ‹è¯•ä»æ•°æ®åº“åŠ è½½æ•°æ®"""
        # Given
        provider = DataProvider()
        connection_string = f"sqlite:///{test_database}"
        query = "SELECT * FROM test_results WHERE test_name = 'APIæµ‹è¯•'"

        # When
        success = provider.load_data_from_database(
            connection_string, query, "db_test_data"
        )

        # Then
        assert success is True

        # éªŒè¯æ•°æ®åŠ è½½
        data = provider.get_next_data("db_test_data")
        assert data is not None
        assert 'response_time' in data
        assert 'success' in data
```

## ğŸŒ ç«¯åˆ°ç«¯æµ‹è¯•

### å®Œæ•´åœºæ™¯æµ‹è¯•

```python
# tests/e2e/test_complete_workflow.py
import pytest
import subprocess
import time
import requests
from pathlib import Path

class TestCompleteWorkflow:
    """å®Œæ•´å·¥ä½œæµç¨‹ç«¯åˆ°ç«¯æµ‹è¯•"""

    @pytest.fixture(scope="class")
    def locust_server(self):
        """å¯åŠ¨LocustæœåŠ¡å™¨"""
        # å¯åŠ¨Locust Web UI
        process = subprocess.Popen([
            'locust',
            '-f', 'tests/e2e/fixtures/test_locustfile.py',
            '--web-host', '127.0.0.1',
            '--web-port', '8089',
            '--headless'
        ])

        # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
        time.sleep(5)

        yield process

        # æ¸…ç†
        process.terminate()
        process.wait()

    @pytest.mark.e2e
    @pytest.mark.slow
    def test_complete_performance_test_workflow(self, locust_server):
        """æµ‹è¯•å®Œæ•´çš„æ€§èƒ½æµ‹è¯•å·¥ä½œæµç¨‹"""
        base_url = "http://127.0.0.1:8089"

        # 1. å¯åŠ¨æµ‹è¯•
        start_response = requests.post(f"{base_url}/swarm", data={
            'user_count': 10,
            'spawn_rate': 2,
            'host': 'http://httpbin.org'
        })
        assert start_response.status_code == 200

        # 2. ç­‰å¾…æµ‹è¯•è¿è¡Œ
        time.sleep(30)

        # 3. æ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯
        stats_response = requests.get(f"{base_url}/stats/requests")
        assert stats_response.status_code == 200

        stats_data = stats_response.json()
        assert 'stats' in stats_data
        assert len(stats_data['stats']) > 0

        # 4. åœæ­¢æµ‹è¯•
        stop_response = requests.get(f"{base_url}/stop")
        assert stop_response.status_code == 200

        # 5. éªŒè¯æŠ¥å‘Šç”Ÿæˆ
        reports_dir = Path("reports")
        assert reports_dir.exists()

        # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†æ€§èƒ½æŠ¥å‘Š
        html_reports = list(reports_dir.glob("*.html"))
        assert len(html_reports) > 0
```

### APIæµ‹è¯•

```python
# tests/e2e/test_api_endpoints.py
import pytest
import requests

class TestAPIEndpoints:
    """APIç«¯ç‚¹ç«¯åˆ°ç«¯æµ‹è¯•"""

    @pytest.fixture
    def api_base_url(self):
        """APIåŸºç¡€URL"""
        return "http://localhost:8089"

    @pytest.mark.e2e
    def test_web_ui_endpoints(self, api_base_url):
        """æµ‹è¯•Web UIç«¯ç‚¹"""
        endpoints = [
            "/",
            "/stats/requests",
            "/stats/distribution",
            "/exceptions"
        ]

        for endpoint in endpoints:
            response = requests.get(f"{api_base_url}{endpoint}")
            assert response.status_code == 200

    @pytest.mark.e2e
    def test_api_workflow(self, api_base_url):
        """æµ‹è¯•APIå·¥ä½œæµç¨‹"""
        # 1. è·å–åˆå§‹çŠ¶æ€
        status_response = requests.get(f"{api_base_url}/stats/requests")
        assert status_response.status_code == 200

        # 2. å¯åŠ¨æµ‹è¯•
        start_data = {
            'user_count': 5,
            'spawn_rate': 1,
            'host': 'http://httpbin.org'
        }
        start_response = requests.post(f"{api_base_url}/swarm", data=start_data)
        assert start_response.status_code == 200

        # 3. æ£€æŸ¥è¿è¡ŒçŠ¶æ€
        time.sleep(10)
        running_stats = requests.get(f"{api_base_url}/stats/requests")
        assert running_stats.status_code == 200

        # 4. åœæ­¢æµ‹è¯•
        stop_response = requests.get(f"{api_base_url}/stop")
        assert stop_response.status_code == 200
```

## ğŸš€ æ€§èƒ½æµ‹è¯•

### åŸºå‡†æµ‹è¯•

```python
# tests/performance/test_benchmarks.py
import pytest
import time
from src.analysis.performance_analyzer import PerformanceAnalyzer

class TestPerformanceBenchmarks:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    @pytest.mark.benchmark
    def test_analysis_performance(self, benchmark):
        """æµ‹è¯•åˆ†ææ€§èƒ½"""
        # Given
        analyzer = PerformanceAnalyzer()
        large_dataset = {
            'requests': [
                {'response_time': i % 1000, 'success': True}
                for i in range(10000)
            ],
            'duration': 600,
            'users': 100
        }

        # When & Then
        result = benchmark(analyzer.comprehensive_analysis, large_dataset)
        assert result is not None

    @pytest.mark.benchmark
    def test_data_processing_performance(self, benchmark):
        """æµ‹è¯•æ•°æ®å¤„ç†æ€§èƒ½"""
        from src.data_manager.data_generator import DataGenerator

        generator = DataGenerator()

        def generate_large_dataset():
            return [generator.generate_user_profile() for _ in range(1000)]

        result = benchmark(generate_large_dataset)
        assert len(result) == 1000
```

### å†…å­˜æµ‹è¯•

```python
# tests/performance/test_memory_usage.py
import pytest
import psutil
import os
from src.analysis.performance_analyzer import PerformanceAnalyzer

class TestMemoryUsage:
    """å†…å­˜ä½¿ç”¨æµ‹è¯•"""

    def get_memory_usage(self):
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡"""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024  # MB

    @pytest.mark.memory
    def test_memory_leak_in_analysis(self):
        """æµ‹è¯•åˆ†æè¿‡ç¨‹ä¸­çš„å†…å­˜æ³„æ¼"""
        analyzer = PerformanceAnalyzer()

        initial_memory = self.get_memory_usage()

        # æ‰§è¡Œå¤šæ¬¡åˆ†æ
        for i in range(100):
            test_data = {
                'requests': [
                    {'response_time': j, 'success': True}
                    for j in range(100)
                ],
                'duration': 60,
                'users': 10
            }
            analyzer.comprehensive_analysis(test_data)

        final_memory = self.get_memory_usage()
        memory_increase = final_memory - initial_memory

        # å†…å­˜å¢é•¿ä¸åº”è¶…è¿‡50MB
        assert memory_increase < 50, f"å†…å­˜æ³„æ¼æ£€æµ‹ï¼šå¢é•¿äº† {memory_increase:.2f}MB"
```

## ğŸ”§ æµ‹è¯•å·¥å…·é…ç½®

### pytesté…ç½®

```ini
# pytest.ini
[tool:pytest]
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*
addopts =
    -v
    --strict-markers
    --strict-config
    --tb=short
    --cov=src
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=80

markers =
    unit: Unit tests
    integration: Integration tests
    e2e: End-to-end tests
    slow: Slow running tests
    smoke: Smoke tests
    regression: Regression tests
    benchmark: Performance benchmark tests
    memory: Memory usage tests
    property: Property-based tests

filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
```

### æµ‹è¯•æ•°æ®ç®¡ç†

```python
# tests/conftest.py
import pytest
import json
from pathlib import Path

@pytest.fixture(scope="session")
def test_data_dir():
    """æµ‹è¯•æ•°æ®ç›®å½•"""
    return Path(__file__).parent / "fixtures"

@pytest.fixture
def sample_test_data(test_data_dir):
    """ç¤ºä¾‹æµ‹è¯•æ•°æ®"""
    data_file = test_data_dir / "sample_test_data.json"
    with open(data_file, 'r', encoding='utf-8') as f:
        return json.load(f)

@pytest.fixture
def temp_reports_dir(tmp_path):
    """ä¸´æ—¶æŠ¥å‘Šç›®å½•"""
    reports_dir = tmp_path / "reports"
    reports_dir.mkdir()
    return reports_dir

# æ•°æ®åº“æµ‹è¯•fixture
@pytest.fixture(scope="session")
def test_database():
    """æµ‹è¯•æ•°æ®åº“"""
    # è®¾ç½®æµ‹è¯•æ•°æ®åº“
    # è¿”å›æ•°æ®åº“è¿æ¥ä¿¡æ¯
    pass
```

## ğŸ“Š æµ‹è¯•æŠ¥å‘Š

### è¦†ç›–ç‡æŠ¥å‘Š

```bash
# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=src --cov-report=html --cov-report=term

# æŸ¥çœ‹HTMLæŠ¥å‘Š
open htmlcov/index.html
```

### æµ‹è¯•ç»“æœåˆ†æ

```python
# tests/utils/test_reporter.py
class TestReporter:
    """æµ‹è¯•ç»“æœæŠ¥å‘Šå™¨"""

    def generate_test_summary(self, test_results):
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        summary = {
            'total_tests': len(test_results),
            'passed': len([t for t in test_results if t.passed]),
            'failed': len([t for t in test_results if t.failed]),
            'skipped': len([t for t in test_results if t.skipped]),
            'coverage': self.calculate_coverage(),
            'duration': sum(t.duration for t in test_results)
        }
        return summary

    def generate_html_report(self, summary, output_path):
        """ç”ŸæˆHTMLæµ‹è¯•æŠ¥å‘Š"""
        # ç”Ÿæˆè¯¦ç»†çš„HTMLæŠ¥å‘Š
        pass
```

## âš ï¸ æµ‹è¯•æœ€ä½³å®è·µ

1. **æµ‹è¯•å‘½å**: ä½¿ç”¨æè¿°æ€§çš„æµ‹è¯•åç§°
2. **æµ‹è¯•éš”ç¦»**: æ¯ä¸ªæµ‹è¯•åº”è¯¥ç‹¬ç«‹è¿è¡Œ
3. **æ•°æ®æ¸…ç†**: æµ‹è¯•åæ¸…ç†ä¸´æ—¶æ•°æ®
4. **Mockä½¿ç”¨**: åˆç†ä½¿ç”¨Mockéš”ç¦»ä¾èµ–
5. **æ–­è¨€æ˜ç¡®**: ä½¿ç”¨æ˜ç¡®çš„æ–­è¨€æ¶ˆæ¯
6. **æ€§èƒ½è€ƒè™‘**: é¿å…æµ‹è¯•è¿è¡Œæ—¶é—´è¿‡é•¿
7. **æŒç»­é›†æˆ**: é›†æˆåˆ°CI/CDæµç¨‹ä¸­

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [å¼€å‘ç¯å¢ƒæ­å»º](setup.md) - æµ‹è¯•ç¯å¢ƒé…ç½®
- [ç¼–ç è§„èŒƒ](coding-standards.md) - æµ‹è¯•ä»£ç è§„èŒƒ
- [è´¡çŒ®æŒ‡å—](contributing.md) - æµ‹è¯•è´¡çŒ®æµç¨‹
- [æ€§èƒ½ä¼˜åŒ–](performance-tuning.md) - æ€§èƒ½æµ‹è¯•æŒ‡å—
