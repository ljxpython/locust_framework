# åˆ†å¸ƒå¼é…ç½®

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å¦‚ä½•é…ç½®Locustæ€§èƒ½æµ‹è¯•æ¡†æ¶çš„åˆ†å¸ƒå¼éƒ¨ç½²ï¼ŒåŒ…æ‹¬Master-Workeræ¶æ„ã€é›†ç¾¤ç®¡ç†å’Œè´Ÿè½½å‡è¡¡é…ç½®ã€‚

## ğŸ—ï¸ åˆ†å¸ƒå¼æ¶æ„

### æ¶æ„æ¦‚è§ˆ

```mermaid
graph TD
    A[è´Ÿè½½å‡è¡¡å™¨] --> B[Locust Master 1]
    A --> C[Locust Master 2]
    A --> D[Locust Master 3]

    B --> E[Worker Group 1]
    B --> F[Worker Group 2]
    C --> G[Worker Group 3]
    C --> H[Worker Group 4]
    D --> I[Worker Group 5]
    D --> J[Worker Group 6]

    E --> K[ç›®æ ‡ç³»ç»Ÿé›†ç¾¤]
    F --> K
    G --> K
    H --> K
    I --> K
    J --> K

    L[ç›‘æ§ç³»ç»Ÿ] --> A
    L --> B
    L --> C
    L --> D
    L --> E
    L --> F
    L --> G
    L --> H
    L --> I
    L --> J
```

### ç»„ä»¶è¯´æ˜

```yaml
# distributed/architecture.yml
components:
  load_balancer:
    description: "è´Ÿè½½å‡è¡¡å™¨ï¼Œåˆ†å‘Web UIè¯·æ±‚"
    technology: "Nginx/HAProxy/AWS ALB"
    ports: [80, 443]

  master_nodes:
    description: "Locust MasterèŠ‚ç‚¹ï¼Œåè°ƒæµ‹è¯•æ‰§è¡Œ"
    count: 3
    ports: [8089, 5557]
    resources:
      cpu: "2 cores"
      memory: "4GB"
      storage: "20GB"

  worker_groups:
    description: "Locust WorkerèŠ‚ç‚¹ç»„ï¼Œæ‰§è¡Œå®é™…æµ‹è¯•"
    groups: 6
    workers_per_group: 10
    ports: []
    resources:
      cpu: "1 core"
      memory: "2GB"
      storage: "10GB"

  target_system:
    description: "è¢«æµ‹è¯•çš„ç›®æ ‡ç³»ç»Ÿ"
    instances: "å¤šå®ä¾‹"
    load_balancer: "ç‹¬ç«‹è´Ÿè½½å‡è¡¡"

  monitoring:
    description: "ç›‘æ§å’Œæ—¥å¿—æ”¶é›†ç³»ç»Ÿ"
    components: ["Prometheus", "Grafana", "ELK Stack"]
```

## ğŸ›ï¸ MasterèŠ‚ç‚¹é…ç½®

### 1. é«˜å¯ç”¨Masteré…ç½®

```yaml
# config/master-ha.yml
master_cluster:
  enabled: true
  nodes:
    - name: "master-1"
      host: "10.0.1.10"
      port: 5557
      web_port: 8089
      priority: 100

    - name: "master-2"
      host: "10.0.1.11"
      port: 5557
      web_port: 8089
      priority: 90

    - name: "master-3"
      host: "10.0.1.12"
      port: 5557
      web_port: 8089
      priority: 80

  # é€‰ä¸¾é…ç½®
  election:
    enabled: true
    timeout: 30
    heartbeat_interval: 5

  # æ•°æ®åŒæ­¥
  synchronization:
    enabled: true
    sync_interval: 10
    conflict_resolution: "latest_wins"

# MasterèŠ‚ç‚¹é…ç½®
master:
  # ç»‘å®šé…ç½®
  bind:
    host: "0.0.0.0"
    port: 5557

  # Web UIé…ç½®
  web:
    host: "0.0.0.0"
    port: 8089
    auth_credentials: "admin:secure_password"

  # Workerç®¡ç†
  workers:
    expect_workers: 60  # æœŸæœ›Workeræ•°é‡
    expect_workers_max_wait: 120  # æœ€å¤§ç­‰å¾…æ—¶é—´
    heartbeat_liveness: 60  # å¿ƒè·³å­˜æ´»æ—¶é—´
    heartbeat_interval: 3  # å¿ƒè·³é—´éš”

  # è´Ÿè½½åˆ†é…
  load_distribution:
    strategy: "round_robin"  # round_robin, least_loaded, weighted
    rebalance_interval: 30

  # æ•…éšœå¤„ç†
  fault_tolerance:
    worker_timeout: 60
    retry_failed_workers: true
    max_retries: 3

  # èµ„æºé™åˆ¶
  resources:
    max_memory_usage: "4GB"
    max_cpu_usage: "80%"
    max_connections: 10000
```

### 2. Masterå¯åŠ¨è„šæœ¬

```bash
#!/bin/bash
# scripts/start-master.sh

set -e

# é…ç½®å˜é‡
MASTER_HOST=${MASTER_HOST:-"0.0.0.0"}
MASTER_PORT=${MASTER_PORT:-5557}
WEB_HOST=${WEB_HOST:-"0.0.0.0"}
WEB_PORT=${WEB_PORT:-8089}
LOCUSTFILE=${LOCUSTFILE:-"locustfiles/distributed.py"}
EXPECT_WORKERS=${EXPECT_WORKERS:-60}

# æ—¥å¿—é…ç½®
LOG_LEVEL=${LOG_LEVEL:-"INFO"}
LOG_FILE=${LOG_FILE:-"/var/log/locust/master.log"}

# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p $(dirname $LOG_FILE)

# å¥åº·æ£€æŸ¥å‡½æ•°
health_check() {
    local max_attempts=30
    local attempt=1

    while [ $attempt -le $max_attempts ]; do
        if curl -f http://${WEB_HOST}:${WEB_PORT}/stats/requests > /dev/null 2>&1; then
            echo "Master is healthy"
            return 0
        fi

        echo "Health check attempt $attempt/$max_attempts failed"
        sleep 2
        ((attempt++))
    done

    echo "Master health check failed"
    return 1
}

# å¯åŠ¨Master
echo "Starting Locust Master..."
echo "Configuration:"
echo "  Host: ${MASTER_HOST}:${MASTER_PORT}"
echo "  Web UI: ${WEB_HOST}:${WEB_PORT}"
echo "  Locustfile: ${LOCUSTFILE}"
echo "  Expected Workers: ${EXPECT_WORKERS}"

# å¯åŠ¨å‘½ä»¤
locust \
    --master \
    --master-bind-host=${MASTER_HOST} \
    --master-bind-port=${MASTER_PORT} \
    --web-host=${WEB_HOST} \
    --web-port=${WEB_PORT} \
    --expect-workers=${EXPECT_WORKERS} \
    --loglevel=${LOG_LEVEL} \
    --logfile=${LOG_FILE} \
    -f ${LOCUSTFILE} &

MASTER_PID=$!
echo "Master started with PID: $MASTER_PID"

# ç­‰å¾…å¯åŠ¨
sleep 10

# å¥åº·æ£€æŸ¥
if health_check; then
    echo "Master is running successfully"
    echo $MASTER_PID > /var/run/locust-master.pid
else
    echo "Master failed to start properly"
    kill $MASTER_PID 2>/dev/null || true
    exit 1
fi

# ä¿æŒå‰å°è¿è¡Œ
wait $MASTER_PID
```

## ğŸ‘¥ WorkerèŠ‚ç‚¹é…ç½®

### 1. Workeré…ç½®

```yaml
# config/worker.yml
worker:
  # Masterè¿æ¥é…ç½®
  master:
    host: "master.locust.local"  # Masterä¸»æœºåæˆ–IP
    port: 5557
    connection_timeout: 30
    reconnect_interval: 5
    max_reconnect_attempts: 10

  # Workeræ ‡è¯†
  identity:
    node_id: "${HOSTNAME}"
    tags: ["region:us-west", "zone:a", "type:standard"]

  # èµ„æºé…ç½®
  resources:
    max_users: 1000  # å•ä¸ªWorkeræœ€å¤§ç”¨æˆ·æ•°
    cpu_limit: "1000m"  # CPUé™åˆ¶
    memory_limit: "2Gi"  # å†…å­˜é™åˆ¶

  # æ€§èƒ½è°ƒä¼˜
  performance:
    gevent_pool_size: 1000
    connection_pool_size: 100
    dns_cache_size: 1000

  # ç›‘æ§é…ç½®
  monitoring:
    metrics_enabled: true
    metrics_port: 9090
    health_check_port: 8080

  # æ—¥å¿—é…ç½®
  logging:
    level: "INFO"
    format: "json"
    file: "/var/log/locust/worker.log"
    max_size: "100MB"
    backup_count: 5
```

### 2. Workerè‡ªåŠ¨æ‰©ç¼©å®¹

```yaml
# k8s/worker-hpa.yml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: locust-worker-hpa
  namespace: locust
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: locust-worker
  minReplicas: 10
  maxReplicas: 100
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: locust_users_per_worker
      target:
        type: AverageValue
        averageValue: "500"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60

---
# Workeréƒ¨ç½²é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: locust-worker
  namespace: locust
spec:
  replicas: 10
  selector:
    matchLabels:
      app: locust-worker
  template:
    metadata:
      labels:
        app: locust-worker
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: locust-worker
        image: locust-framework:latest
        ports:
        - containerPort: 9090
          name: metrics
        - containerPort: 8080
          name: health
        env:
        - name: LOCUST_MODE
          value: "worker"
        - name: LOCUST_MASTER_HOST
          value: "locust-master-service"
        - name: LOCUST_MASTER_PORT
          value: "5557"
        - name: WORKER_NODE_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: locustfiles
          mountPath: /app/locustfiles
        - name: config
          mountPath: /app/config
      volumes:
      - name: locustfiles
        configMap:
          name: locustfiles-config
      - name: config
        configMap:
          name: worker-config
```

### 3. Workerå¯åŠ¨è„šæœ¬

```bash
#!/bin/bash
# scripts/start-worker.sh

set -e

# é…ç½®å˜é‡
MASTER_HOST=${MASTER_HOST:-"master.locust.local"}
MASTER_PORT=${MASTER_PORT:-5557}
LOCUSTFILE=${LOCUSTFILE:-"locustfiles/distributed.py"}
WORKER_NODE_ID=${WORKER_NODE_ID:-$(hostname)}

# æ—¥å¿—é…ç½®
LOG_LEVEL=${LOG_LEVEL:-"INFO"}
LOG_FILE=${LOG_FILE:-"/var/log/locust/worker-${WORKER_NODE_ID}.log"}

# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p $(dirname $LOG_FILE)

# Masterè¿æ¥æ£€æŸ¥
check_master_connection() {
    local max_attempts=30
    local attempt=1

    while [ $attempt -le $max_attempts ]; do
        if nc -z ${MASTER_HOST} ${MASTER_PORT}; then
            echo "Master connection successful"
            return 0
        fi

        echo "Master connection attempt $attempt/$max_attempts failed"
        sleep 2
        ((attempt++))
    done

    echo "Failed to connect to Master"
    return 1
}

# å¯åŠ¨Worker
echo "Starting Locust Worker..."
echo "Configuration:"
echo "  Worker ID: ${WORKER_NODE_ID}"
echo "  Master: ${MASTER_HOST}:${MASTER_PORT}"
echo "  Locustfile: ${LOCUSTFILE}"

# æ£€æŸ¥Masterè¿æ¥
if ! check_master_connection; then
    echo "Cannot connect to Master, exiting"
    exit 1
fi

# å¯åŠ¨å‘½ä»¤
exec locust \
    --worker \
    --master-host=${MASTER_HOST} \
    --master-port=${MASTER_PORT} \
    --loglevel=${LOG_LEVEL} \
    --logfile=${LOG_FILE} \
    -f ${LOCUSTFILE}
```

## ğŸ”„ è´Ÿè½½å‡è¡¡é…ç½®

### 1. Nginxè´Ÿè½½å‡è¡¡

```nginx
# nginx/locust-lb.conf
upstream locust_masters {
    least_conn;
    server 10.0.1.10:8089 weight=3 max_fails=3 fail_timeout=30s;
    server 10.0.1.11:8089 weight=2 max_fails=3 fail_timeout=30s;
    server 10.0.1.12:8089 weight=1 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    listen 443 ssl http2;
    server_name locust.company.com;

    # SSLé…ç½®
    ssl_certificate /etc/ssl/certs/locust.crt;
    ssl_certificate_key /etc/ssl/private/locust.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;

    # å®‰å…¨å¤´
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";

    # ä»£ç†é…ç½®
    location / {
        proxy_pass http://locust_masters;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # WebSocketæ”¯æŒ
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";

        # è¶…æ—¶é…ç½®
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;

        # ç¼“å†²é…ç½®
        proxy_buffering off;
        proxy_request_buffering off;
    }

    # å¥åº·æ£€æŸ¥ç«¯ç‚¹
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }

    # é™æ€èµ„æºç¼“å­˜
    location ~* \.(css|js|png|jpg|jpeg|gif|ico|svg)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
```

### 2. HAProxyé…ç½®

```haproxy
# haproxy/haproxy.cfg
global
    daemon
    maxconn 4096
    log stdout local0

defaults
    mode http
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms
    option httplog
    option dontlognull

# ç»Ÿè®¡é¡µé¢
stats enable
stats uri /stats
stats refresh 30s
stats admin if TRUE

# Locust Masterè´Ÿè½½å‡è¡¡
frontend locust_frontend
    bind *:80
    bind *:443 ssl crt /etc/ssl/certs/locust.pem
    redirect scheme https if !{ ssl_fc }

    # ACLè§„åˆ™
    acl is_websocket hdr(Upgrade) -i websocket
    acl is_api path_beg /api
    acl is_stats path_beg /stats

    # è·¯ç”±è§„åˆ™
    use_backend locust_masters if !is_stats
    use_backend stats_backend if is_stats

backend locust_masters
    balance leastconn
    option httpchk GET /stats/requests

    server master1 10.0.1.10:8089 check weight 3
    server master2 10.0.1.11:8089 check weight 2
    server master3 10.0.1.12:8089 check weight 1

backend stats_backend
    stats enable
    stats uri /stats
    stats refresh 30s
```

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### 1. Prometheusç›‘æ§é…ç½®

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "locust_rules.yml"

scrape_configs:
  # Locust Masterç›‘æ§
  - job_name: 'locust-master'
    static_configs:
      - targets:
        - 'master-1:9090'
        - 'master-2:9090'
        - 'master-3:9090'
    scrape_interval: 10s
    metrics_path: '/metrics'

  # Locust Workerç›‘æ§
  - job_name: 'locust-worker'
    kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
            - locust
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: locust-worker
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: (.+)
        replacement: ${1}:9090

  # ç³»ç»Ÿç›‘æ§
  - job_name: 'node-exporter'
    static_configs:
      - targets:
        - 'node1:9100'
        - 'node2:9100'
        - 'node3:9100'

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - 'alertmanager:9093'
```

### 2. å‘Šè­¦è§„åˆ™

```yaml
# monitoring/locust_rules.yml
groups:
  - name: locust_alerts
    rules:
      # MasterèŠ‚ç‚¹å‘Šè­¦
      - alert: LocustMasterDown
        expr: up{job="locust-master"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Locust MasterèŠ‚ç‚¹å®•æœº"
          description: "MasterèŠ‚ç‚¹ {{ $labels.instance }} å·²å®•æœºè¶…è¿‡1åˆ†é’Ÿ"

      # Workerè¿æ¥å‘Šè­¦
      - alert: LocustWorkerDisconnected
        expr: locust_workers_connected < 50
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Locust Workerè¿æ¥æ•°è¿‡ä½"
          description: "å½“å‰è¿æ¥çš„Workeræ•°é‡ä¸º {{ $value }}ï¼Œä½äºé¢„æœŸ"

      # æ€§èƒ½å‘Šè­¦
      - alert: LocustHighErrorRate
        expr: locust_errors_rate > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Locusté”™è¯¯ç‡è¿‡é«˜"
          description: "é”™è¯¯ç‡ä¸º {{ $value | humanizePercentage }}ï¼Œè¶…è¿‡5%"

      # èµ„æºå‘Šè­¦
      - alert: LocustHighMemoryUsage
        expr: process_resident_memory_bytes{job=~"locust-.*"} / 1024 / 1024 / 1024 > 1.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Locustå†…å­˜ä½¿ç”¨è¿‡é«˜"
          description: "{{ $labels.instance }} å†…å­˜ä½¿ç”¨é‡ä¸º {{ $value | humanize }}GB"
```

## ğŸš€ éƒ¨ç½²è„šæœ¬

### 1. å®Œæ•´éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# scripts/deploy-distributed.sh

set -e

# é…ç½®å˜é‡
NAMESPACE=${NAMESPACE:-"locust"}
MASTER_REPLICAS=${MASTER_REPLICAS:-3}
WORKER_REPLICAS=${WORKER_REPLICAS:-60}
IMAGE_TAG=${IMAGE_TAG:-"latest"}

echo "Deploying Locust distributed cluster..."
echo "Namespace: $NAMESPACE"
echo "Master replicas: $MASTER_REPLICAS"
echo "Worker replicas: $WORKER_REPLICAS"
echo "Image tag: $IMAGE_TAG"

# åˆ›å»ºå‘½åç©ºé—´
kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

# éƒ¨ç½²é…ç½®
kubectl apply -f k8s/configmaps/ -n $NAMESPACE
kubectl apply -f k8s/secrets/ -n $NAMESPACE

# éƒ¨ç½²Master
envsubst < k8s/master-deployment.yml | kubectl apply -f - -n $NAMESPACE
kubectl apply -f k8s/master-service.yml -n $NAMESPACE

# ç­‰å¾…Masterå¯åŠ¨
echo "Waiting for Master to be ready..."
kubectl wait --for=condition=available --timeout=300s deployment/locust-master -n $NAMESPACE

# éƒ¨ç½²Worker
envsubst < k8s/worker-deployment.yml | kubectl apply -f - -n $NAMESPACE

# éƒ¨ç½²HPA
kubectl apply -f k8s/worker-hpa.yml -n $NAMESPACE

# éƒ¨ç½²ç›‘æ§
kubectl apply -f monitoring/ -n $NAMESPACE

# éƒ¨ç½²Ingress
kubectl apply -f k8s/ingress.yml -n $NAMESPACE

echo "Deployment completed!"
echo "Access Locust Web UI at: https://locust.company.com"

# æ˜¾ç¤ºçŠ¶æ€
kubectl get all -n $NAMESPACE
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [ç”Ÿäº§ç¯å¢ƒé…ç½®](production.md) - ç”Ÿäº§éƒ¨ç½²æŒ‡å—
- [ç›‘æ§é…ç½®](monitoring-config.md) - ç›‘æ§ç³»ç»Ÿé…ç½®
- [æ•…éšœæ’é™¤](../examples/troubleshooting.md) - åˆ†å¸ƒå¼é—®é¢˜æ’æŸ¥
- [æ€§èƒ½ä¼˜åŒ–](../examples/best-practices.md) - æ€§èƒ½ä¼˜åŒ–å»ºè®®
